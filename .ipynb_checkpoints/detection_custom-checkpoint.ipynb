{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare custom datasets for object detection\n===============================================\n\nWith GluonCV, we have already provided built-in support for widely used public datasets with zero\neffort, e.g. `sphx_glr_build_examples_datasets_pascal_voc.py` and `sphx_glr_build_examples_datasets_mscoco.py`.\n\nHowever it is very natural to create a custom dataset of your choice for object detection tasks.\n\nThis tutorial is intend to provide you some hints to clear the path for you.\nIn practice, feel free to choose whatever method that fits for your use case best.\n\n`lst_record_dataset`\n\n`pascal_voc_like`\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n1. Preferred Object Detection Format for GluonCV and MXNet\n----------------------------------------------------------\nLet us walk through some fundamental backgrounds in case you are not familiar with them.\n\nBounding Boxes\n^^^^^^^^^^^^^^\n\nThere are multiple ways to organize the label format for object detection task. We will briefly introduce the\nmost widely used: ``bounding box``.\n\nGluonCV expect all bounding boxes to be encoded as (xmin, ymin, xmax, ymax), aka (left, top, right, bottom) borders of each object of interest.\n\nFirst of all, let us plot a real image for example:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, zipfile\nfrom gluoncv import utils\nimport mxnet as mx\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nim_fname = utils.download('https://github.com/dmlc/web-data/blob/master/' +\n                          'gluoncv/datasets/dog.jpg?raw=true',\n                          path='dog.jpg')\nimg = mx.image.imread(im_fname)\nax = utils.viz.plot_image(img)\nprint(img.shape)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's label the image manually for demo.\n\n.. hint::\n\n   In practice, a dedicated GUI labeling tool is more convenient.\n\nWe expect all bounding boxes follow this format: (xmin, ymin, xmax, ymax)\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dog_label = [130, 220, 320, 530]\nbike_label = [115, 120, 580, 420]\ncar_label = [480, 80, 700, 170]\nall_boxes = np.array([dog_label, bike_label, car_label])\nall_ids = np.array([0, 1, 2])\nclass_names = ['dog', 'bike', 'car']\n\n# see how it looks by rendering the boxes into image\nax = utils.viz.plot_bbox(img, all_boxes, labels=all_ids, class_names=class_names)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LST Label for GluonCV and MXNet\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFollowing the convention used in MXNet, we recommand a LST file which is a plain text list file to store labels.\n\nLST file was first introduced in MXNet following the `RecordIO design <https://mxnet.incubator.apache.org/architecture/note_data_loading.html>`_ and the `List file tutorial <https://mxnet.incubator.apache.org/faq/recordio.html>`_ of creating a LST file.\n\n.. hint::\n\n  The benefits of using single LST file are two fold:\n\n  1. It's easier to manege single file rather than scattered annotation files.\n\n  2. It's compatible with ``RecordFile`` binary format which we will cover in this tutorial later.\n\nThe format of LST file is:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\ninteger_image_index \\t label_of_variable_length \\t relative_path_to_image\n\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we take the list of names of all images, shuffles them, then separates them into two lists: a training filename list and a testing filename list.\n\nHere we use compatible format for object detection task as `mxnet.image.ImageDetIter <https://mxnet.apache.org/api/python/image/image.html#image-iterator-for-object-detection>`_.\n\n`mxnet.image.ImageDetIter` is a object detection data iterator written in C++ which includes tons of augmentation choices. However, it's not flexible enough to handle all kinds of customized data augmentation.\nAs a result, in GluonCV, we switched to :py:mod:`gluoncv.data.transforms` to support almost all types of data augmentations.\n\nMore specifically, the label of object detection task is described as follows:\n\n![](https://github.com/dmlc/web-data/blob/master/gluoncv/datasets/detection_label.png?raw=true)\n\n\n![](https://github.com/dmlc/web-data/blob/master/gluoncv/datasets/detection_label_detail.png?raw=true)\n\n\nSo, the corresponding LST file for the image we just labeled can be formatted as:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_line(img_path, im_shape, boxes, ids, idx):\n    h, w, c = im_shape\n    # for header, we use minimal length 2, plus width and height\n    # with A: 4, B: 5, C: width, D: height\n    A = 4\n    B = 5\n    C = w\n    D = h\n    # concat id and bboxes\n    labels = np.hstack((ids.reshape(-1, 1), boxes)).astype('float')\n    # normalized bboxes (recommanded)\n    labels[:, (1, 3)] /= float(w)\n    labels[:, (2, 4)] /= float(h)\n    # flatten\n    labels = labels.flatten().tolist()\n    str_idx = [str(idx)]\n    str_header = [str(x) for x in [A, B, C, D]]\n    str_labels = [str(x) for x in labels]\n    str_path = [img_path]\n    line = '\\t'.join(str_idx + str_header + str_labels + str_path) + '\\n'\n    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single line may be long, but contains complete information of each image required by object detection.\n\nThe length of each line varies, depending on how many objects are labeled inside the corresponding image.\n\nBy stacking lines one by one, it is very nature to create ``train.lst`` and ``val.lst`` for training/validation purposes.\n\nIn this tutorial, we repeat the same image 4 times to create a fake ``val.lst`` file.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('val.lst', 'w') as fw:\n    for i in range(4):\n        line = write_line('dog.jpg', img.shape, all_boxes, all_ids, i)\n        print(line)\n        fw.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LstDetection for Loading Raw Images in Folders\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAssume the relative root path to the image folder is current directory\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gluoncv.data import LstDetection\nlst_dataset = LstDetection('val.lst', root=os.path.expanduser('.'))\nprint('length:', len(lst_dataset))\nfirst_img = lst_dataset[0][0]\nprint('image shape:', first_img.shape)\nprint('Label example:')\nprint(lst_dataset[0][1])\nprint(\"GluonCV swaps bounding boxes to columns 0-3 by default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RecordFileDetection for Entire Dataset Packed in Signle MXNet RecordFile\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nReading scattered images in folders can be slow, due to constraint of disk random access speed.\nThere's a significant gap between random/sequential access speed especially on HDDs.\nEven with modern PCI-E based Solid State Drives, sequential reading IO performance still blows\nrandom reading by a large margin.\n\nWe will skip repeating the design of RecordIO built into MXNet, if you are interested, have a look at `RecordIO design <https://mxnet.incubator.apache.org/architecture/note_data_loading.html>`_.\n\nIn this section, we go through the fundamental steps to create a record file.\n\nFirst of all, you will need a ``im2rec.py`` file to start with.\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. hint::\n\n     You can find `im2rec.py` in `incubator-mxnet/tools/ <https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py>`_, or you can simply download it now.\n\n     Usage:\n\n     .. code-block:: bash\n\n         python im2rec.py lst_file_name relative_root_to_images --pass-through --pack-label\n\n     Some important arguments to the ``im2rec.py``:\n\n          - ``--pass-through``: no transcode of original image, pack it to binary as is. It will preserve original quality and aspect ratio anyway.\n\n          - ``--pack-label``: pack the labels in lst file to binary record file, so ``.rec`` file is self compelete.\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\nimport subprocess\nim2rec = utils.download('https://raw.githubusercontent.com/apache/incubator-mxnet/' +\n                        '6843914f642c8343aaa9a09db803b6af6f5d94a2/tools/im2rec.py', 'im2rec.py')\nsubprocess.check_output([sys.executable, 'im2rec.py', 'val', '.', '--no-shuffle', '--pass-through', '--pack-label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now similarly, we can create a dataset from the binary file we just created with on line of code:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gluoncv.data import RecordFileDetection\nrecord_dataset = RecordFileDetection('val.rec', coord_normalized=True)\n\n# we expect same results from LstDetection\nprint('length:', len(record_dataset))\nfirst_img = record_dataset[0][0]\nprint('image shape:', first_img.shape)\nprint('Label example:')\nprint(record_dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n2. Derive from PASCAL VOC format\n--------------------------------\nIt you have a custom dataset fully comply with the `Pascal VOC <http://host.robots.ox.ac.uk/pascal/VOC/>`_ object detection format,\nthat could be good news, because it's can be adapted to GluonCV format real quick.\n\nWe provide a template for you to peek the structures\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname = utils.download('https://github.com/dmlc/web-data/blob/master/gluoncv/datasets/VOCtemplate.zip?raw=true', 'VOCtemplate.zip')\nwith zipfile.ZipFile(fname) as zf:\n    zf.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A VOC-like dataset will have the following structure:\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\nVOCtemplate\n└── VOC2018\n    ├── Annotations\n    │   └── 000001.xml\n    ├── ImageSets\n    │   └── Main\n    │       └── train.txt\n    └── JPEGImages\n        └── 000001.jpg\n\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an example of annotation file:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('VOCtemplate/VOC2018/Annotations/000001.xml', 'r') as fid:\n    print(fid.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as your dataset can match the PASCAL VOC convension, it is convenient to\nderive custom dataset from ``VOCDetection``\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gluoncv.data import VOCDetection\nclass VOCLike(VOCDetection):\n    CLASSES = ['person', 'dog']\n    def __init__(self, root, splits, transform=None, index_map=None, preload_label=True):\n        super(VOCLike, self).__init__(root, splits, transform, index_map, preload_label)\n\ndataset = VOCLike(root='VOCtemplate', splits=((2018, 'train'),))\nprint('length of dataset:', len(dataset))\nprint('label example:')\nprint(dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last column indicate the difficulties of labeled object\nYou can ignore the following section if it's out of your intention in the xml file:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"<difficult>0</difficult>\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
